{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2\n",
    "#### Due Mar. 18th by end of day. Name your notebook as firstname.lastname.HW2.ipynb and email it to zhang@csc.lsu.edu\n",
    "\n",
    "Your tasks in this homework are to experiment with CNN.\n",
    "A simple CNN is given below. \n",
    "\n",
    "### Task 1\n",
    "Train the CNN (as much as you can) to reach convergence. Investigate what patterns the first layer (layer 0) filters pick up by plotting the filters as small 2d images. To plot a 2d array x as image, use \"imshow(x, cmap=cm.gray)\". You should plot the 10 filters together using subplot. \n",
    "\n",
    "### Task 2\n",
    "The given CNN has 2 conv&pool layers, 1 hidden layer and 1 output layer. \n",
    "Modify the CNN to have:\n",
    "  - 1 conv&pool layer, 1 hidden layer and 1 output layer;\n",
    "  - 1 hidden layer and 1 output layer; \n",
    "  \n",
    "while keeping the other parameters the same. Compare the error rates on the test data for the original CNN and the two modifications and determine whether the conv&pool layers play a significant role for performance.\n",
    "\n",
    "### Task 3\n",
    "Change the number of filters for the two conv&pool layers:\n",
    "  - try 10 filters for layer 1 and 20 for layer 2;\n",
    "  - try 20 filters for layer 1 and 10 for layer 2.\n",
    "\n",
    "Compare error rate of the two cases and that of the original. Comment on how number of filters can impact performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle, gzip\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer and Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "\n",
    "        W_values = 4*numpy.random.uniform(\n",
    "                low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        self.b = theano.shared(value=numpy.zeros((n_out,)), name='b', borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        net = T.dot(self.input, self.W) + self.b\n",
    "        self.output = T.nnet.sigmoid(net)\n",
    "\n",
    "        \n",
    "class MultiLogisticRegression(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros((n_in, n_out)),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value = numpy.zeros((n_out,)),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.prob = T.nnet.softmax(T.dot(self.input, self.W) + self.b)\n",
    "        self.predict = T.argmax(self.prob, axis=1)\n",
    "\n",
    "    def nll(self, y):\n",
    "        return  -T.mean(T.log(self.prob)[T.arange(y.shape[0]), y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv + Pool Layer\n",
    "### output size = (imagesize - filtersize + 1)/poolsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvPoolLayer(object):\n",
    "\n",
    "    def __init__(self, input, filter_shape, image_shape, poolsize):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        n_in = filter_shape[1]*filter_shape[2]*filter_shape[3]\n",
    "        n_out = (filter_shape[0]*filter_shape[2]*filter_shape[3])/(poolsize[0]*poolsize[1])\n",
    "        W_bound = numpy.sqrt(6./(n_in + n_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.random.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(value=numpy.zeros((filter_shape[0],)), borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        conv_out = conv.conv2d(\n",
    "            input=self.input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making ConvNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "learning_rate=0.1\n",
    "nkerns=[10, 10]\n",
    "\n",
    "\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "\n",
    "\n",
    "layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "layer0 = ConvPoolLayer(\n",
    "    input=layer0_input,\n",
    "    image_shape=(batch_size, 1, 28, 28),\n",
    "    filter_shape=(nkerns[0], 1, 5, 5),\n",
    "    poolsize=(2, 2)\n",
    ")\n",
    "\n",
    "layer1 = ConvPoolLayer(\n",
    "    input=layer0.output,\n",
    "    image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "    filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "    poolsize=(2, 2)\n",
    ")\n",
    "layer1_output = layer1.output.flatten(2)\n",
    "\n",
    "layer2 = HiddenLayer(\n",
    "    input=layer1_output,\n",
    "    n_in=nkerns[1]*4*4,\n",
    "    n_out=50,\n",
    ")\n",
    "\n",
    "layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "\n",
    "cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "model_predict = theano.function(\n",
    "    [x],\n",
    "    layer3.predict\n",
    ")\n",
    "\n",
    "\n",
    "params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "grads = T.grad(cost, params)\n",
    "updates = [\n",
    "    (param_i, param_i - learning_rate * grad_i)\n",
    "    for param_i, grad_i in zip(params, grads)\n",
    "]\n",
    "\n",
    "train_model = theano.function(\n",
    "    [x, y],\n",
    "    cost,\n",
    "    updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = 'digits.pkl.gz' \n",
    "f = gzip.open(dataset, 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "train_set_x, train_set_y = train_set\n",
    "test_set_x, test_set_y = test_set\n",
    "train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "ix = []\n",
    "for i in range(10):\n",
    "    ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "ix = numpy.concatenate(ix)\n",
    "train_set_x = train_set_x[ix]\n",
    "train_set_y = train_set_y[ix]\n",
    "ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "train_set_x = train_set_x[ix]\n",
    "train_set_y = train_set_y[ix]\n",
    "\n",
    "n_batches = train_set_x.shape[0]\n",
    "n_batches /= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 , nll = 45.4577066948\n",
      "iteration: 1 , nll = 43.002593448\n",
      "iteration: 2 , nll = 38.5016274663\n",
      "iteration: 3 , nll = 33.3300802424\n",
      "iteration: 4 , nll = 28.6615564057\n",
      "iteration: 5 , nll = 24.8656978454\n",
      "iteration: 6 , nll = 21.8948217045\n",
      "iteration: 7 , nll = 19.578643201\n",
      "iteration: 8 , nll = 17.7518963497\n",
      "iteration: 9 , nll = 16.2838616541\n",
      "iteration: 10 , nll = 15.0771506121\n",
      "iteration: 11 , nll = 14.06568468\n",
      "iteration: 12 , nll = 13.200879399\n",
      "iteration: 13 , nll = 12.4507680368\n",
      "iteration: 14 , nll = 11.7898808176\n",
      "iteration: 15 , nll = 11.2008356862\n",
      "iteration: 16 , nll = 10.670825536\n",
      "iteration: 17 , nll = 10.1904141078\n",
      "iteration: 18 , nll = 9.75414203143\n",
      "iteration: 19 , nll = 9.35604335373\n",
      "iteration: 20 , nll = 8.99077209626\n",
      "iteration: 21 , nll = 8.6545966109\n",
      "iteration: 22 , nll = 8.34419534523\n",
      "iteration: 23 , nll = 8.05573341574\n",
      "iteration: 24 , nll = 7.7872146756\n",
      "iteration: 25 , nll = 7.53700912701\n",
      "iteration: 26 , nll = 7.30287316606\n",
      "iteration: 27 , nll = 7.08327806973\n",
      "iteration: 28 , nll = 6.876536356\n",
      "iteration: 29 , nll = 6.68222710613\n",
      "iteration: 30 , nll = 6.49906686813\n",
      "iteration: 31 , nll = 6.32485369306\n",
      "iteration: 32 , nll = 6.16036499206\n",
      "iteration: 33 , nll = 6.00418966405\n",
      "iteration: 34 , nll = 5.85556553915\n",
      "iteration: 35 , nll = 5.7145195524\n",
      "iteration: 36 , nll = 5.58063757421\n",
      "iteration: 37 , nll = 5.4525285877\n",
      "iteration: 38 , nll = 5.33040446569\n",
      "iteration: 39 , nll = 5.21334818392\n",
      "iteration: 40 , nll = 5.1012852234\n",
      "iteration: 41 , nll = 4.99337186425\n",
      "iteration: 42 , nll = 4.88985405781\n",
      "iteration: 43 , nll = 4.78995284478\n",
      "iteration: 44 , nll = 4.69410613367\n",
      "iteration: 45 , nll = 4.60110426881\n",
      "iteration: 46 , nll = 4.51175138028\n",
      "iteration: 47 , nll = 4.42558079145\n",
      "iteration: 48 , nll = 4.34224819885\n",
      "iteration: 49 , nll = 4.2616606973\n",
      "iteration: 50 , nll = 4.18425129792\n",
      "iteration: 51 , nll = 4.10942094188\n",
      "iteration: 52 , nll = 4.03722422807\n",
      "iteration: 53 , nll = 3.9673150136\n",
      "iteration: 54 , nll = 3.8999799905\n",
      "iteration: 55 , nll = 3.83409184796\n",
      "iteration: 56 , nll = 3.77054447907\n",
      "iteration: 57 , nll = 3.70849574356\n",
      "iteration: 58 , nll = 3.64794471731\n",
      "iteration: 59 , nll = 3.58908771771\n",
      "iteration: 60 , nll = 3.5318123135\n",
      "iteration: 61 , nll = 3.47654779727\n",
      "iteration: 62 , nll = 3.42244641855\n",
      "iteration: 63 , nll = 3.36980809486\n",
      "iteration: 64 , nll = 3.31859723917\n",
      "iteration: 65 , nll = 3.2687071296\n",
      "iteration: 66 , nll = 3.22027189343\n",
      "iteration: 67 , nll = 3.1730271132\n",
      "iteration: 68 , nll = 3.12689166349\n",
      "iteration: 69 , nll = 3.08192524716\n",
      "iteration: 70 , nll = 3.0381360564\n",
      "iteration: 71 , nll = 2.99532815777\n",
      "iteration: 72 , nll = 2.95364813337\n",
      "iteration: 73 , nll = 2.91291637331\n",
      "iteration: 74 , nll = 2.87325505904\n",
      "iteration: 75 , nll = 2.83466978774\n",
      "iteration: 76 , nll = 2.79653364993\n",
      "iteration: 77 , nll = 2.75965526938\n",
      "iteration: 78 , nll = 2.7232781598\n",
      "iteration: 79 , nll = 2.68787031463\n",
      "iteration: 80 , nll = 2.65331031394\n",
      "iteration: 81 , nll = 2.61960483419\n",
      "iteration: 82 , nll = 2.58623148641\n",
      "iteration: 83 , nll = 2.55379517371\n",
      "iteration: 84 , nll = 2.52196011165\n",
      "iteration: 85 , nll = 2.49065491725\n",
      "iteration: 86 , nll = 2.46033545048\n",
      "iteration: 87 , nll = 2.43049528639\n",
      "iteration: 88 , nll = 2.40107270923\n",
      "iteration: 89 , nll = 2.37240927654\n",
      "iteration: 90 , nll = 2.34400494288\n",
      "iteration: 91 , nll = 2.31618655436\n",
      "iteration: 92 , nll = 2.2888731995\n",
      "iteration: 93 , nll = 2.26174292118\n",
      "iteration: 94 , nll = 2.23531158562\n",
      "iteration: 95 , nll = 2.20942965677\n",
      "iteration: 96 , nll = 2.18395396186\n",
      "iteration: 97 , nll = 2.1591467391\n",
      "iteration: 98 , nll = 2.13460556336\n",
      "iteration: 99 , nll = 2.11052572501\n",
      "iteration: 100 , nll = 2.08702450914\n",
      "iteration: 101 , nll = 2.06392176442\n",
      "iteration: 102 , nll = 2.04090342092\n",
      "iteration: 103 , nll = 2.01865411387\n",
      "iteration: 104 , nll = 1.99660367495\n",
      "iteration: 105 , nll = 1.97484083288\n",
      "iteration: 106 , nll = 1.95357667841\n",
      "iteration: 107 , nll = 1.93270783724\n",
      "iteration: 108 , nll = 1.91205954444\n",
      "iteration: 109 , nll = 1.89203208017\n",
      "iteration: 110 , nll = 1.87211349583\n",
      "iteration: 111 , nll = 1.85263830647\n",
      "iteration: 112 , nll = 1.83347616938\n",
      "iteration: 113 , nll = 1.81456511766\n",
      "iteration: 114 , nll = 1.79584481522\n",
      "iteration: 115 , nll = 1.77734014517\n",
      "iteration: 116 , nll = 1.75912135252\n",
      "iteration: 117 , nll = 1.74152122661\n",
      "iteration: 118 , nll = 1.72393515392\n",
      "iteration: 119 , nll = 1.7063674095\n",
      "iteration: 120 , nll = 1.68955129876\n",
      "iteration: 121 , nll = 1.67278567511\n",
      "iteration: 122 , nll = 1.65605487759\n",
      "iteration: 123 , nll = 1.63967782727\n",
      "iteration: 124 , nll = 1.62355122433\n",
      "iteration: 125 , nll = 1.60780195609\n",
      "iteration: 126 , nll = 1.59205798141\n",
      "iteration: 127 , nll = 1.5767978085\n",
      "iteration: 128 , nll = 1.5615529717\n",
      "iteration: 129 , nll = 1.54652498871\n",
      "iteration: 130 , nll = 1.53185605206\n",
      "iteration: 131 , nll = 1.51680303461\n",
      "iteration: 132 , nll = 1.5027553438\n",
      "iteration: 133 , nll = 1.48829964044\n",
      "iteration: 134 , nll = 1.47395698583\n",
      "iteration: 135 , nll = 1.46026379431\n",
      "iteration: 136 , nll = 1.44637020485\n",
      "iteration: 137 , nll = 1.43292734775\n",
      "iteration: 138 , nll = 1.41942669892\n",
      "iteration: 139 , nll = 1.40633452909\n",
      "iteration: 140 , nll = 1.3931265564\n",
      "iteration: 141 , nll = 1.38024678835\n",
      "iteration: 142 , nll = 1.36774921193\n",
      "iteration: 143 , nll = 1.35507399132\n",
      "iteration: 144 , nll = 1.34255949039\n",
      "iteration: 145 , nll = 1.33056758828\n",
      "iteration: 146 , nll = 1.31849947756\n",
      "iteration: 147 , nll = 1.3066541123\n",
      "iteration: 148 , nll = 1.2950293461\n",
      "iteration: 149 , nll = 1.28345144528\n",
      "iteration: 150 , nll = 1.27229383669\n",
      "iteration: 151 , nll = 1.26073102534\n",
      "iteration: 152 , nll = 1.24982173079\n",
      "iteration: 153 , nll = 1.23880917575\n",
      "iteration: 154 , nll = 1.22775261481\n",
      "iteration: 155 , nll = 1.2170271364\n",
      "iteration: 156 , nll = 1.20652599843\n",
      "iteration: 157 , nll = 1.19598237719\n",
      "iteration: 158 , nll = 1.18564708363\n",
      "iteration: 159 , nll = 1.17530389946\n",
      "iteration: 160 , nll = 1.16543284522\n",
      "iteration: 161 , nll = 1.15557016281\n",
      "iteration: 162 , nll = 1.1457435372\n",
      "iteration: 163 , nll = 1.13610915518\n",
      "iteration: 164 , nll = 1.12663023405\n",
      "iteration: 165 , nll = 1.11735729435\n",
      "iteration: 166 , nll = 1.10806533501\n",
      "iteration: 167 , nll = 1.0990234273\n",
      "iteration: 168 , nll = 1.08993602353\n",
      "iteration: 169 , nll = 1.08105307061\n",
      "iteration: 170 , nll = 1.07225145377\n",
      "iteration: 171 , nll = 1.06358044085\n",
      "iteration: 172 , nll = 1.05491704892\n",
      "iteration: 173 , nll = 1.04643722982\n",
      "iteration: 174 , nll = 1.03794736704\n",
      "iteration: 175 , nll = 1.02967693002\n",
      "iteration: 176 , nll = 1.02152816078\n",
      "iteration: 177 , nll = 1.01344909476\n",
      "iteration: 178 , nll = 1.00539827597\n",
      "iteration: 179 , nll = 0.99764136394\n",
      "iteration: 180 , nll = 0.989643976682\n",
      "iteration: 181 , nll = 0.981838544968\n",
      "iteration: 182 , nll = 0.9742971497\n",
      "iteration: 183 , nll = 0.966776693478\n",
      "iteration: 184 , nll = 0.959241916725\n",
      "iteration: 185 , nll = 0.951769395713\n",
      "iteration: 186 , nll = 0.944615792508\n",
      "iteration: 187 , nll = 0.937329056368\n",
      "iteration: 188 , nll = 0.930091405245\n",
      "iteration: 189 , nll = 0.923126062278\n",
      "iteration: 190 , nll = 0.916162329951\n",
      "iteration: 191 , nll = 0.909052159888\n",
      "iteration: 192 , nll = 0.902341777156\n",
      "iteration: 193 , nll = 0.895579624691\n",
      "iteration: 194 , nll = 0.888733411182\n",
      "iteration: 195 , nll = 0.882180549865\n",
      "iteration: 196 , nll = 0.875612163931\n",
      "iteration: 197 , nll = 0.869147906044\n",
      "iteration: 198 , nll = 0.862796613834\n",
      "iteration: 199 , nll = 0.856424532277\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "c = numpy.zeros((n_epochs,))\n",
    "for i in range(n_epochs): \n",
    "    err = 0\n",
    "    for b in range(n_batches):\n",
    "        err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "    print 'iteration:', i, ', nll =', err\n",
    "    c[i] = err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10a47e890>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108309490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0384\n"
     ]
    }
   ],
   "source": [
    "n_testbatches = test_set_x.shape[0] / batch_size\n",
    "err = 0\n",
    "for b in range(n_testbatches):\n",
    "    yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "    yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "    err += len(np.nonzero(yp - yy)[0])\n",
    "\n",
    "print 1.0*err/len(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
