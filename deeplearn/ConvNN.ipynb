{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2\n",
    "#### Due Mar. 18th by end of day. Name your notebook as firstname.lastname.HW2.ipynb and email it to zhang@csc.lsu.edu\n",
    "\n",
    "Your tasks in this homework are to experiment with CNN.\n",
    "A simple CNN is given below. \n",
    "\n",
    "### Task 1\n",
    "Train the CNN (as much as you can) to reach convergence. Investigate what patterns the first layer (layer 0) filters pick up by plotting the filters as small 2d images. To plot a 2d array x as image, use \"imshow(x, cmap=cm.gray)\". You should plot the 10 filters together using subplot. \n",
    "\n",
    "### Task 2\n",
    "The given CNN has 2 conv&pool layers, 1 hidden layer and 1 output layer. \n",
    "Modify the CNN to have:\n",
    "  - 1 conv&pool layer, 1 hidden layer and 1 output layer;\n",
    "  - 1 hidden layer and 1 output layer; \n",
    "  \n",
    "while keeping the other parameters the same. Compare the error rates on the test data for the original CNN and the two modifications and determine whether the conv&pool layers play a significant role for performance.\n",
    "\n",
    "### Task 3\n",
    "Change the number of filters for the two conv&pool layers:\n",
    "  - try 10 filters for layer 1 and 20 for layer 2;\n",
    "  - try 20 filters for layer 1 and 10 for layer 2.\n",
    "\n",
    "Compare error rate of the two cases and that of the original. Comment on how number of filters can impact performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle, gzip\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer and Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "\n",
    "        W_values = 4*numpy.random.uniform(\n",
    "                low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        self.b = theano.shared(value=numpy.zeros((n_out,)), name='b', borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        net = T.dot(self.input, self.W) + self.b\n",
    "        self.output = T.nnet.sigmoid(net)\n",
    "\n",
    "        \n",
    "class MultiLogisticRegression(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros((n_in, n_out)),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value = numpy.zeros((n_out,)),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.prob = T.nnet.softmax(T.dot(self.input, self.W) + self.b)\n",
    "        self.predict = T.argmax(self.prob, axis=1)\n",
    "\n",
    "    def nll(self, y):\n",
    "        return  -T.mean(T.log(self.prob)[T.arange(y.shape[0]), y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv + Pool Layer\n",
    "### output size = (imagesize - filtersize + 1)/poolsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvPoolLayer(object):\n",
    "\n",
    "    def __init__(self, input, filter_shape, image_shape, poolsize):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        n_in = filter_shape[1]*filter_shape[2]*filter_shape[3]\n",
    "        n_out = (filter_shape[0]*filter_shape[2]*filter_shape[3])/(poolsize[0]*poolsize[1])\n",
    "        W_bound = numpy.sqrt(6./(n_in + n_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.random.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(value=numpy.zeros((filter_shape[0],)), borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        conv_out = conv.conv2d(\n",
    "            input=self.input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making ConvNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "learning_rate=0.1\n",
    "nkerns=[10, 10]\n",
    "\n",
    "\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "\n",
    "\n",
    "layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "layer0 = ConvPoolLayer(\n",
    "    input=layer0_input,\n",
    "    image_shape=(batch_size, 1, 28, 28),\n",
    "    filter_shape=(nkerns[0], 1, 5, 5),\n",
    "    poolsize=(2, 2)\n",
    ")\n",
    "\n",
    "layer1 = ConvPoolLayer(\n",
    "    input=layer0.output,\n",
    "    image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "    filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "    poolsize=(2, 2)\n",
    ")\n",
    "layer1_output = layer1.output.flatten(2)\n",
    "\n",
    "layer2 = HiddenLayer(\n",
    "    input=layer1_output,\n",
    "    n_in=nkerns[1]*4*4,\n",
    "    n_out=50,\n",
    ")\n",
    "\n",
    "layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "\n",
    "cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "model_predict = theano.function(\n",
    "    [x],\n",
    "    layer3.predict\n",
    ")\n",
    "\n",
    "\n",
    "params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "grads = T.grad(cost, params)\n",
    "updates = [\n",
    "    (param_i, param_i - learning_rate * grad_i)\n",
    "    for param_i, grad_i in zip(params, grads)\n",
    "]\n",
    "\n",
    "train_model = theano.function(\n",
    "    [x, y],\n",
    "    cost,\n",
    "    updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = 'digits.pkl.gz' \n",
    "f = gzip.open(dataset, 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "train_set_x, train_set_y = train_set\n",
    "test_set_x, test_set_y = test_set\n",
    "train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "ix = []\n",
    "for i in range(10):\n",
    "    ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "ix = numpy.concatenate(ix)\n",
    "train_set_x = train_set_x[ix]\n",
    "train_set_y = train_set_y[ix]\n",
    "ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "train_set_x = train_set_x[ix]\n",
    "train_set_y = train_set_y[ix]\n",
    "\n",
    "n_batches = train_set_x.shape[0]\n",
    "n_batches /= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 , nll = 45.1780980383\n",
      "iteration: 1 , nll = 42.2615095687\n",
      "iteration: 2 , nll = 37.8040743204\n",
      "iteration: 3 , nll = 33.1011214281\n",
      "iteration: 4 , nll = 29.0138154716\n",
      "iteration: 5 , nll = 25.6802799256\n",
      "iteration: 6 , nll = 23.0052467993\n",
      "iteration: 7 , nll = 20.8391973134\n",
      "iteration: 8 , nll = 19.0561584066\n",
      "iteration: 9 , nll = 17.5590079206\n",
      "iteration: 10 , nll = 16.283151117\n",
      "iteration: 11 , nll = 15.1805239315\n",
      "iteration: 12 , nll = 14.2154197146\n",
      "iteration: 13 , nll = 13.3623298149\n",
      "iteration: 14 , nll = 12.6024355814\n",
      "iteration: 15 , nll = 11.9212642106\n",
      "iteration: 16 , nll = 11.3069533892\n",
      "iteration: 17 , nll = 10.751162823\n",
      "iteration: 18 , nll = 10.2460590778\n",
      "iteration: 19 , nll = 9.78226781244\n",
      "iteration: 20 , nll = 9.35597807886\n",
      "iteration: 21 , nll = 8.96396846219\n",
      "iteration: 22 , nll = 8.60249229624\n",
      "iteration: 23 , nll = 8.26728404315\n",
      "iteration: 24 , nll = 7.95679097629\n",
      "iteration: 25 , nll = 7.66767357431\n",
      "iteration: 26 , nll = 7.3978538939\n",
      "iteration: 27 , nll = 7.14582847725\n",
      "iteration: 28 , nll = 6.9104598899\n",
      "iteration: 29 , nll = 6.6901054753\n",
      "iteration: 30 , nll = 6.48395572524\n",
      "iteration: 31 , nll = 6.29017146472\n",
      "iteration: 32 , nll = 6.10734705777\n",
      "iteration: 33 , nll = 5.93507018908\n",
      "iteration: 34 , nll = 5.77290083938\n",
      "iteration: 35 , nll = 5.61964634965\n",
      "iteration: 36 , nll = 5.47405304637\n",
      "iteration: 37 , nll = 5.33606260734\n",
      "iteration: 38 , nll = 5.20464707211\n",
      "iteration: 39 , nll = 5.08030563312\n",
      "iteration: 40 , nll = 4.96128611154\n",
      "iteration: 41 , nll = 4.84837987251\n",
      "iteration: 42 , nll = 4.74007628912\n",
      "iteration: 43 , nll = 4.63630872519\n",
      "iteration: 44 , nll = 4.53657611915\n",
      "iteration: 45 , nll = 4.44109186406\n",
      "iteration: 46 , nll = 4.34906673565\n",
      "iteration: 47 , nll = 4.26065002202\n",
      "iteration: 48 , nll = 4.17596137318\n",
      "iteration: 49 , nll = 4.09435744192\n",
      "iteration: 50 , nll = 4.01586002983\n",
      "iteration: 51 , nll = 3.9400453751\n",
      "iteration: 52 , nll = 3.86686549974\n",
      "iteration: 53 , nll = 3.79663710531\n",
      "iteration: 54 , nll = 3.72845904664\n",
      "iteration: 55 , nll = 3.66243207465\n",
      "iteration: 56 , nll = 3.59869053156\n",
      "iteration: 57 , nll = 3.53624246905\n",
      "iteration: 58 , nll = 3.47617938157\n",
      "iteration: 59 , nll = 3.41772095243\n",
      "iteration: 60 , nll = 3.3606562681\n",
      "iteration: 61 , nll = 3.30530582507\n",
      "iteration: 62 , nll = 3.25133739731\n",
      "iteration: 63 , nll = 3.19901306125\n",
      "iteration: 64 , nll = 3.14835135241\n",
      "iteration: 65 , nll = 3.09912659158\n",
      "iteration: 66 , nll = 3.05100085058\n",
      "iteration: 67 , nll = 3.00393995496\n",
      "iteration: 68 , nll = 2.95842663349\n",
      "iteration: 69 , nll = 2.91383614706\n",
      "iteration: 70 , nll = 2.86992625523\n",
      "iteration: 71 , nll = 2.82736715932\n",
      "iteration: 72 , nll = 2.78560296614\n",
      "iteration: 73 , nll = 2.74458390305\n",
      "iteration: 74 , nll = 2.70493725451\n",
      "iteration: 75 , nll = 2.66619102293\n",
      "iteration: 76 , nll = 2.62778494361\n",
      "iteration: 77 , nll = 2.59118719214\n",
      "iteration: 78 , nll = 2.55489480542\n",
      "iteration: 79 , nll = 2.51962869442\n",
      "iteration: 80 , nll = 2.48478866929\n",
      "iteration: 81 , nll = 2.45097798129\n",
      "iteration: 82 , nll = 2.41754570264\n",
      "iteration: 83 , nll = 2.38522156631\n",
      "iteration: 84 , nll = 2.35337274192\n",
      "iteration: 85 , nll = 2.32214180995\n",
      "iteration: 86 , nll = 2.29173217398\n",
      "iteration: 87 , nll = 2.26231695419\n",
      "iteration: 88 , nll = 2.23295803346\n",
      "iteration: 89 , nll = 2.20446054374\n",
      "iteration: 90 , nll = 2.17688350217\n",
      "iteration: 91 , nll = 2.14941261523\n",
      "iteration: 92 , nll = 2.12278427442\n",
      "iteration: 93 , nll = 2.09667787333\n",
      "iteration: 94 , nll = 2.07131275962\n",
      "iteration: 95 , nll = 2.04625971474\n",
      "iteration: 96 , nll = 2.02182044708\n",
      "iteration: 97 , nll = 1.99811810306\n",
      "iteration: 98 , nll = 1.97460045563\n",
      "iteration: 99 , nll = 1.95170521466\n",
      "iteration: 100 , nll = 1.9291634401\n",
      "iteration: 101 , nll = 1.90687666575\n",
      "iteration: 102 , nll = 1.8851685101\n",
      "iteration: 103 , nll = 1.86374876324\n",
      "iteration: 104 , nll = 1.84252013675\n",
      "iteration: 105 , nll = 1.82213549428\n",
      "iteration: 106 , nll = 1.801837218\n",
      "iteration: 107 , nll = 1.782247881\n",
      "iteration: 108 , nll = 1.76272589558\n",
      "iteration: 109 , nll = 1.74357397088\n",
      "iteration: 110 , nll = 1.72445366058\n",
      "iteration: 111 , nll = 1.7057244902\n",
      "iteration: 112 , nll = 1.68771965299\n",
      "iteration: 113 , nll = 1.66995204837\n",
      "iteration: 114 , nll = 1.65252686159\n",
      "iteration: 115 , nll = 1.63551213286\n",
      "iteration: 116 , nll = 1.61875116309\n",
      "iteration: 117 , nll = 1.60212408333\n",
      "iteration: 118 , nll = 1.58538799363\n",
      "iteration: 119 , nll = 1.56966351751\n",
      "iteration: 120 , nll = 1.55379872372\n",
      "iteration: 121 , nll = 1.53812271722\n",
      "iteration: 122 , nll = 1.52268664785\n",
      "iteration: 123 , nll = 1.50749790678\n",
      "iteration: 124 , nll = 1.4926437694\n",
      "iteration: 125 , nll = 1.47766333992\n",
      "iteration: 126 , nll = 1.46338660724\n",
      "iteration: 127 , nll = 1.44898200922\n",
      "iteration: 128 , nll = 1.4348776706\n",
      "iteration: 129 , nll = 1.42105252488\n",
      "iteration: 130 , nll = 1.4071741152\n",
      "iteration: 131 , nll = 1.39376634468\n",
      "iteration: 132 , nll = 1.38046751415\n",
      "iteration: 133 , nll = 1.36744109754\n",
      "iteration: 134 , nll = 1.35465976434\n",
      "iteration: 135 , nll = 1.34229168561\n",
      "iteration: 136 , nll = 1.32973789241\n",
      "iteration: 137 , nll = 1.31749733604\n",
      "iteration: 138 , nll = 1.30543571764\n",
      "iteration: 139 , nll = 1.29351252797\n",
      "iteration: 140 , nll = 1.28190621424\n",
      "iteration: 141 , nll = 1.27023307018\n",
      "iteration: 142 , nll = 1.25907746511\n",
      "iteration: 143 , nll = 1.24761646263\n",
      "iteration: 144 , nll = 1.23668515562\n",
      "iteration: 145 , nll = 1.22570357623\n",
      "iteration: 146 , nll = 1.21500091011\n",
      "iteration: 147 , nll = 1.20468633164\n",
      "iteration: 148 , nll = 1.19408651553\n",
      "iteration: 149 , nll = 1.18383060628\n",
      "iteration: 150 , nll = 1.17375402737\n",
      "iteration: 151 , nll = 1.16376478805\n",
      "iteration: 152 , nll = 1.1539260217\n",
      "iteration: 153 , nll = 1.14414204262\n",
      "iteration: 154 , nll = 1.13432342294\n",
      "iteration: 155 , nll = 1.12491119472\n",
      "iteration: 156 , nll = 1.11539386015\n",
      "iteration: 157 , nll = 1.10597629701\n",
      "iteration: 158 , nll = 1.09684584191\n",
      "iteration: 159 , nll = 1.08791437581\n",
      "iteration: 160 , nll = 1.07895452938\n",
      "iteration: 161 , nll = 1.06996070736\n",
      "iteration: 162 , nll = 1.06108061671\n",
      "iteration: 163 , nll = 1.0526781217\n",
      "iteration: 164 , nll = 1.04396689263\n",
      "iteration: 165 , nll = 1.03529478318\n",
      "iteration: 166 , nll = 1.02691530766\n",
      "iteration: 167 , nll = 1.01859677055\n",
      "iteration: 168 , nll = 1.01076689559\n",
      "iteration: 169 , nll = 1.00238619831\n",
      "iteration: 170 , nll = 0.994718785014\n",
      "iteration: 171 , nll = 0.986730628552\n",
      "iteration: 172 , nll = 0.97928661799\n",
      "iteration: 173 , nll = 0.971394807\n",
      "iteration: 174 , nll = 0.963510100401\n",
      "iteration: 175 , nll = 0.956256511175\n",
      "iteration: 176 , nll = 0.948815039984\n",
      "iteration: 177 , nll = 0.941545132386\n",
      "iteration: 178 , nll = 0.934422837588\n",
      "iteration: 179 , nll = 0.927302540287\n",
      "iteration: 180 , nll = 0.920292262093\n",
      "iteration: 181 , nll = 0.913468088814\n",
      "iteration: 182 , nll = 0.906550304935\n",
      "iteration: 183 , nll = 0.899808397589\n",
      "iteration: 184 , nll = 0.892943015428\n",
      "iteration: 185 , nll = 0.886528498227\n",
      "iteration: 186 , nll = 0.879897914628\n",
      "iteration: 187 , nll = 0.873364986751\n",
      "iteration: 188 , nll = 0.867128580425\n",
      "iteration: 189 , nll = 0.860675825622\n",
      "iteration: 190 , nll = 0.85441966969\n",
      "iteration: 191 , nll = 0.848240964033\n",
      "iteration: 192 , nll = 0.842073600746\n",
      "iteration: 193 , nll = 0.835983823375\n",
      "iteration: 194 , nll = 0.830002056808\n",
      "iteration: 195 , nll = 0.824101945293\n",
      "iteration: 196 , nll = 0.81829406397\n",
      "iteration: 197 , nll = 0.812551777229\n",
      "iteration: 198 , nll = 0.806725054055\n",
      "iteration: 199 , nll = 0.800981618042\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "c = numpy.zeros((n_epochs,))\n",
    "for i in range(n_epochs): \n",
    "    err = 0\n",
    "    for b in range(n_batches):\n",
    "        err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "    print 'iteration:', i, ', nll =', err\n",
    "    c[i] = err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['plot', 'f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xaed4efcc>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSJEmSJEkqpf8HWDuTO2lSoHsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaeb0048c>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "plot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0383\n"
     ]
    }
   ],
   "source": [
    "n_testbatches = test_set_x.shape[0] / batch_size\n",
    "err = 0\n",
    "for b in range(n_testbatches):\n",
    "    yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "    yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "    err += len(np.nonzero(yp - yy)[0])\n",
    "\n",
    "print 1.0*err/len(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
